{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env pyhon\n",
    "from __future__ import print_function\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as Data\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cby/pytorch_learning\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "input_size=784\n",
    "hidden_size=500\n",
    "num_classes=10\n",
    "num_epochs=5\n",
    "batch_size=100\n",
    "learning_rate=0.001\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', train=True,transform=transforms.ToTensor(),download=True)\n",
    "test_dataset = dsets.MNIST(root='./data',train=False,transform=transforms.ToTensor())\n",
    "path1=os.path.abspath('.')\n",
    "print(path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_loader=Data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)\n",
    "print(type(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=500, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_classes):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1=torch.nn.Linear(input_size,hidden_size)\n",
    "        self.relu=torch.nn.ReLU()\n",
    "        self.fc2=torch.nn.Linear(hidden_size,num_classes)\n",
    "    def forward(self,x):\n",
    "        out=self.fc1(x)\n",
    "        out=self.relu(out)\n",
    "        out=self.fc2(out)\n",
    "        return out\n",
    "\n",
    "net = Net(input_size,hidden_size,num_classes)\n",
    "print(net)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cirterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step[10/600], Loss: 1.4278\n",
      "Epoch [1/5], Step[20/600], Loss: 0.7800\n",
      "Epoch [1/5], Step[30/600], Loss: 0.6541\n",
      "Epoch [1/5], Step[40/600], Loss: 0.3430\n",
      "Epoch [1/5], Step[50/600], Loss: 0.4542\n",
      "Epoch [1/5], Step[60/600], Loss: 0.2433\n",
      "Epoch [1/5], Step[70/600], Loss: 0.3607\n",
      "Epoch [1/5], Step[80/600], Loss: 0.3966\n",
      "Epoch [1/5], Step[90/600], Loss: 0.3582\n",
      "Epoch [1/5], Step[100/600], Loss: 0.4051\n",
      "Epoch [1/5], Step[110/600], Loss: 0.2773\n",
      "Epoch [1/5], Step[120/600], Loss: 0.2965\n",
      "Epoch [1/5], Step[130/600], Loss: 0.3407\n",
      "Epoch [1/5], Step[140/600], Loss: 0.4576\n",
      "Epoch [1/5], Step[150/600], Loss: 0.3709\n",
      "Epoch [1/5], Step[160/600], Loss: 0.3330\n",
      "Epoch [1/5], Step[170/600], Loss: 0.3169\n",
      "Epoch [1/5], Step[180/600], Loss: 0.2467\n",
      "Epoch [1/5], Step[190/600], Loss: 0.3511\n",
      "Epoch [1/5], Step[200/600], Loss: 0.1727\n",
      "Epoch [1/5], Step[210/600], Loss: 0.2813\n",
      "Epoch [1/5], Step[220/600], Loss: 0.3976\n",
      "Epoch [1/5], Step[230/600], Loss: 0.2672\n",
      "Epoch [1/5], Step[240/600], Loss: 0.1242\n",
      "Epoch [1/5], Step[250/600], Loss: 0.3306\n",
      "Epoch [1/5], Step[260/600], Loss: 0.2857\n",
      "Epoch [1/5], Step[270/600], Loss: 0.1589\n",
      "Epoch [1/5], Step[280/600], Loss: 0.1205\n",
      "Epoch [1/5], Step[290/600], Loss: 0.2234\n",
      "Epoch [1/5], Step[300/600], Loss: 0.1899\n",
      "Epoch [1/5], Step[310/600], Loss: 0.2106\n",
      "Epoch [1/5], Step[320/600], Loss: 0.2449\n",
      "Epoch [1/5], Step[330/600], Loss: 0.2501\n",
      "Epoch [1/5], Step[340/600], Loss: 0.1236\n",
      "Epoch [1/5], Step[350/600], Loss: 0.2573\n",
      "Epoch [1/5], Step[360/600], Loss: 0.1376\n",
      "Epoch [1/5], Step[370/600], Loss: 0.2906\n",
      "Epoch [1/5], Step[380/600], Loss: 0.1417\n",
      "Epoch [1/5], Step[390/600], Loss: 0.2217\n",
      "Epoch [1/5], Step[400/600], Loss: 0.1664\n",
      "Epoch [1/5], Step[410/600], Loss: 0.1868\n",
      "Epoch [1/5], Step[420/600], Loss: 0.1279\n",
      "Epoch [1/5], Step[430/600], Loss: 0.1425\n",
      "Epoch [1/5], Step[440/600], Loss: 0.2395\n",
      "Epoch [1/5], Step[450/600], Loss: 0.1467\n",
      "Epoch [1/5], Step[460/600], Loss: 0.1145\n",
      "Epoch [1/5], Step[470/600], Loss: 0.1215\n",
      "Epoch [1/5], Step[480/600], Loss: 0.1719\n",
      "Epoch [1/5], Step[490/600], Loss: 0.1560\n",
      "Epoch [1/5], Step[500/600], Loss: 0.1464\n",
      "Epoch [1/5], Step[510/600], Loss: 0.1782\n",
      "Epoch [1/5], Step[520/600], Loss: 0.1335\n",
      "Epoch [1/5], Step[530/600], Loss: 0.2033\n",
      "Epoch [1/5], Step[540/600], Loss: 0.2314\n",
      "Epoch [1/5], Step[550/600], Loss: 0.0711\n",
      "Epoch [1/5], Step[560/600], Loss: 0.0641\n",
      "Epoch [1/5], Step[570/600], Loss: 0.1834\n",
      "Epoch [1/5], Step[580/600], Loss: 0.1567\n",
      "Epoch [1/5], Step[590/600], Loss: 0.1357\n",
      "Epoch [1/5], Step[600/600], Loss: 0.1561\n",
      "Epoch [2/5], Step[10/600], Loss: 0.1776\n",
      "Epoch [2/5], Step[20/600], Loss: 0.1524\n",
      "Epoch [2/5], Step[30/600], Loss: 0.0819\n",
      "Epoch [2/5], Step[40/600], Loss: 0.1037\n",
      "Epoch [2/5], Step[50/600], Loss: 0.0860\n",
      "Epoch [2/5], Step[60/600], Loss: 0.1623\n",
      "Epoch [2/5], Step[70/600], Loss: 0.1915\n",
      "Epoch [2/5], Step[80/600], Loss: 0.0734\n",
      "Epoch [2/5], Step[90/600], Loss: 0.0962\n",
      "Epoch [2/5], Step[100/600], Loss: 0.1140\n",
      "Epoch [2/5], Step[110/600], Loss: 0.1530\n",
      "Epoch [2/5], Step[120/600], Loss: 0.1083\n",
      "Epoch [2/5], Step[130/600], Loss: 0.1762\n",
      "Epoch [2/5], Step[140/600], Loss: 0.1458\n",
      "Epoch [2/5], Step[150/600], Loss: 0.1386\n",
      "Epoch [2/5], Step[160/600], Loss: 0.1173\n",
      "Epoch [2/5], Step[170/600], Loss: 0.1321\n",
      "Epoch [2/5], Step[180/600], Loss: 0.1838\n",
      "Epoch [2/5], Step[190/600], Loss: 0.1466\n",
      "Epoch [2/5], Step[200/600], Loss: 0.0993\n",
      "Epoch [2/5], Step[210/600], Loss: 0.1936\n",
      "Epoch [2/5], Step[220/600], Loss: 0.1463\n",
      "Epoch [2/5], Step[230/600], Loss: 0.1117\n",
      "Epoch [2/5], Step[240/600], Loss: 0.0476\n",
      "Epoch [2/5], Step[250/600], Loss: 0.0858\n",
      "Epoch [2/5], Step[260/600], Loss: 0.0919\n",
      "Epoch [2/5], Step[270/600], Loss: 0.1845\n",
      "Epoch [2/5], Step[280/600], Loss: 0.1250\n",
      "Epoch [2/5], Step[290/600], Loss: 0.1009\n",
      "Epoch [2/5], Step[300/600], Loss: 0.1097\n",
      "Epoch [2/5], Step[310/600], Loss: 0.1833\n",
      "Epoch [2/5], Step[320/600], Loss: 0.1374\n",
      "Epoch [2/5], Step[330/600], Loss: 0.3032\n",
      "Epoch [2/5], Step[340/600], Loss: 0.0675\n",
      "Epoch [2/5], Step[350/600], Loss: 0.0558\n",
      "Epoch [2/5], Step[360/600], Loss: 0.1881\n",
      "Epoch [2/5], Step[370/600], Loss: 0.2084\n",
      "Epoch [2/5], Step[380/600], Loss: 0.0789\n",
      "Epoch [2/5], Step[390/600], Loss: 0.1460\n",
      "Epoch [2/5], Step[400/600], Loss: 0.0817\n",
      "Epoch [2/5], Step[410/600], Loss: 0.1356\n",
      "Epoch [2/5], Step[420/600], Loss: 0.0815\n",
      "Epoch [2/5], Step[430/600], Loss: 0.0394\n",
      "Epoch [2/5], Step[440/600], Loss: 0.0608\n",
      "Epoch [2/5], Step[450/600], Loss: 0.0560\n",
      "Epoch [2/5], Step[460/600], Loss: 0.1011\n",
      "Epoch [2/5], Step[470/600], Loss: 0.0649\n",
      "Epoch [2/5], Step[480/600], Loss: 0.0864\n",
      "Epoch [2/5], Step[490/600], Loss: 0.0330\n",
      "Epoch [2/5], Step[500/600], Loss: 0.0880\n",
      "Epoch [2/5], Step[510/600], Loss: 0.0774\n",
      "Epoch [2/5], Step[520/600], Loss: 0.0327\n",
      "Epoch [2/5], Step[530/600], Loss: 0.1194\n",
      "Epoch [2/5], Step[540/600], Loss: 0.0634\n",
      "Epoch [2/5], Step[550/600], Loss: 0.2355\n",
      "Epoch [2/5], Step[560/600], Loss: 0.1651\n",
      "Epoch [2/5], Step[570/600], Loss: 0.0280\n",
      "Epoch [2/5], Step[580/600], Loss: 0.0787\n",
      "Epoch [2/5], Step[590/600], Loss: 0.0502\n",
      "Epoch [2/5], Step[600/600], Loss: 0.1258\n",
      "Epoch [3/5], Step[10/600], Loss: 0.1760\n",
      "Epoch [3/5], Step[20/600], Loss: 0.0399\n",
      "Epoch [3/5], Step[30/600], Loss: 0.0566\n",
      "Epoch [3/5], Step[40/600], Loss: 0.0743\n",
      "Epoch [3/5], Step[50/600], Loss: 0.0467\n",
      "Epoch [3/5], Step[60/600], Loss: 0.0366\n",
      "Epoch [3/5], Step[70/600], Loss: 0.1744\n",
      "Epoch [3/5], Step[80/600], Loss: 0.0218\n",
      "Epoch [3/5], Step[90/600], Loss: 0.0608\n",
      "Epoch [3/5], Step[100/600], Loss: 0.0318\n",
      "Epoch [3/5], Step[110/600], Loss: 0.2009\n",
      "Epoch [3/5], Step[120/600], Loss: 0.0413\n",
      "Epoch [3/5], Step[130/600], Loss: 0.0920\n",
      "Epoch [3/5], Step[140/600], Loss: 0.0983\n",
      "Epoch [3/5], Step[150/600], Loss: 0.0633\n",
      "Epoch [3/5], Step[160/600], Loss: 0.0203\n",
      "Epoch [3/5], Step[170/600], Loss: 0.0581\n",
      "Epoch [3/5], Step[180/600], Loss: 0.0279\n",
      "Epoch [3/5], Step[190/600], Loss: 0.0441\n",
      "Epoch [3/5], Step[200/600], Loss: 0.0597\n",
      "Epoch [3/5], Step[210/600], Loss: 0.0502\n",
      "Epoch [3/5], Step[220/600], Loss: 0.0534\n",
      "Epoch [3/5], Step[230/600], Loss: 0.0440\n",
      "Epoch [3/5], Step[240/600], Loss: 0.0569\n",
      "Epoch [3/5], Step[250/600], Loss: 0.0742\n",
      "Epoch [3/5], Step[260/600], Loss: 0.0282\n",
      "Epoch [3/5], Step[270/600], Loss: 0.1237\n",
      "Epoch [3/5], Step[280/600], Loss: 0.0194\n",
      "Epoch [3/5], Step[290/600], Loss: 0.0258\n",
      "Epoch [3/5], Step[300/600], Loss: 0.1392\n",
      "Epoch [3/5], Step[310/600], Loss: 0.0354\n",
      "Epoch [3/5], Step[320/600], Loss: 0.0510\n",
      "Epoch [3/5], Step[330/600], Loss: 0.0915\n",
      "Epoch [3/5], Step[340/600], Loss: 0.2170\n",
      "Epoch [3/5], Step[350/600], Loss: 0.0734\n",
      "Epoch [3/5], Step[360/600], Loss: 0.0637\n",
      "Epoch [3/5], Step[370/600], Loss: 0.0763\n",
      "Epoch [3/5], Step[380/600], Loss: 0.1303\n",
      "Epoch [3/5], Step[390/600], Loss: 0.0917\n",
      "Epoch [3/5], Step[400/600], Loss: 0.1191\n",
      "Epoch [3/5], Step[410/600], Loss: 0.0368\n",
      "Epoch [3/5], Step[420/600], Loss: 0.0569\n",
      "Epoch [3/5], Step[430/600], Loss: 0.1257\n",
      "Epoch [3/5], Step[440/600], Loss: 0.0659\n",
      "Epoch [3/5], Step[450/600], Loss: 0.0220\n",
      "Epoch [3/5], Step[460/600], Loss: 0.0931\n",
      "Epoch [3/5], Step[470/600], Loss: 0.0625\n",
      "Epoch [3/5], Step[480/600], Loss: 0.0758\n",
      "Epoch [3/5], Step[490/600], Loss: 0.0690\n",
      "Epoch [3/5], Step[500/600], Loss: 0.0946\n",
      "Epoch [3/5], Step[510/600], Loss: 0.0709\n",
      "Epoch [3/5], Step[520/600], Loss: 0.1300\n",
      "Epoch [3/5], Step[530/600], Loss: 0.1869\n",
      "Epoch [3/5], Step[540/600], Loss: 0.0505\n",
      "Epoch [3/5], Step[550/600], Loss: 0.0743\n",
      "Epoch [3/5], Step[560/600], Loss: 0.0813\n",
      "Epoch [3/5], Step[570/600], Loss: 0.1153\n",
      "Epoch [3/5], Step[580/600], Loss: 0.0567\n",
      "Epoch [3/5], Step[590/600], Loss: 0.0829\n",
      "Epoch [3/5], Step[600/600], Loss: 0.0923\n",
      "Epoch [4/5], Step[10/600], Loss: 0.0627\n",
      "Epoch [4/5], Step[20/600], Loss: 0.0339\n",
      "Epoch [4/5], Step[30/600], Loss: 0.0878\n",
      "Epoch [4/5], Step[40/600], Loss: 0.0752\n",
      "Epoch [4/5], Step[50/600], Loss: 0.2091\n",
      "Epoch [4/5], Step[60/600], Loss: 0.0845\n",
      "Epoch [4/5], Step[70/600], Loss: 0.0505\n",
      "Epoch [4/5], Step[80/600], Loss: 0.0877\n",
      "Epoch [4/5], Step[90/600], Loss: 0.0503\n",
      "Epoch [4/5], Step[100/600], Loss: 0.0410\n",
      "Epoch [4/5], Step[110/600], Loss: 0.0281\n",
      "Epoch [4/5], Step[120/600], Loss: 0.0354\n",
      "Epoch [4/5], Step[130/600], Loss: 0.0499\n",
      "Epoch [4/5], Step[140/600], Loss: 0.1010\n",
      "Epoch [4/5], Step[150/600], Loss: 0.0379\n",
      "Epoch [4/5], Step[160/600], Loss: 0.0511\n",
      "Epoch [4/5], Step[170/600], Loss: 0.1126\n",
      "Epoch [4/5], Step[180/600], Loss: 0.0662\n",
      "Epoch [4/5], Step[190/600], Loss: 0.0200\n",
      "Epoch [4/5], Step[200/600], Loss: 0.0184\n",
      "Epoch [4/5], Step[210/600], Loss: 0.0333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step[220/600], Loss: 0.0674\n",
      "Epoch [4/5], Step[230/600], Loss: 0.0618\n",
      "Epoch [4/5], Step[240/600], Loss: 0.0862\n",
      "Epoch [4/5], Step[250/600], Loss: 0.0724\n",
      "Epoch [4/5], Step[260/600], Loss: 0.0269\n",
      "Epoch [4/5], Step[270/600], Loss: 0.0407\n",
      "Epoch [4/5], Step[280/600], Loss: 0.0170\n",
      "Epoch [4/5], Step[290/600], Loss: 0.0385\n",
      "Epoch [4/5], Step[300/600], Loss: 0.0996\n",
      "Epoch [4/5], Step[310/600], Loss: 0.0764\n",
      "Epoch [4/5], Step[320/600], Loss: 0.1616\n",
      "Epoch [4/5], Step[330/600], Loss: 0.0832\n",
      "Epoch [4/5], Step[340/600], Loss: 0.0905\n",
      "Epoch [4/5], Step[350/600], Loss: 0.0155\n",
      "Epoch [4/5], Step[360/600], Loss: 0.0336\n",
      "Epoch [4/5], Step[370/600], Loss: 0.0115\n",
      "Epoch [4/5], Step[380/600], Loss: 0.0255\n",
      "Epoch [4/5], Step[390/600], Loss: 0.0304\n",
      "Epoch [4/5], Step[400/600], Loss: 0.1012\n",
      "Epoch [4/5], Step[410/600], Loss: 0.0436\n",
      "Epoch [4/5], Step[420/600], Loss: 0.0584\n",
      "Epoch [4/5], Step[430/600], Loss: 0.0217\n",
      "Epoch [4/5], Step[440/600], Loss: 0.0217\n",
      "Epoch [4/5], Step[450/600], Loss: 0.1071\n",
      "Epoch [4/5], Step[460/600], Loss: 0.0956\n",
      "Epoch [4/5], Step[470/600], Loss: 0.0895\n",
      "Epoch [4/5], Step[480/600], Loss: 0.0568\n",
      "Epoch [4/5], Step[490/600], Loss: 0.1352\n",
      "Epoch [4/5], Step[500/600], Loss: 0.0418\n",
      "Epoch [4/5], Step[510/600], Loss: 0.0683\n",
      "Epoch [4/5], Step[520/600], Loss: 0.0892\n",
      "Epoch [4/5], Step[530/600], Loss: 0.0508\n",
      "Epoch [4/5], Step[540/600], Loss: 0.0482\n",
      "Epoch [4/5], Step[550/600], Loss: 0.0441\n",
      "Epoch [4/5], Step[560/600], Loss: 0.1325\n",
      "Epoch [4/5], Step[570/600], Loss: 0.0327\n",
      "Epoch [4/5], Step[580/600], Loss: 0.0443\n",
      "Epoch [4/5], Step[590/600], Loss: 0.0480\n",
      "Epoch [4/5], Step[600/600], Loss: 0.0488\n",
      "Epoch [5/5], Step[10/600], Loss: 0.0075\n",
      "Epoch [5/5], Step[20/600], Loss: 0.1044\n",
      "Epoch [5/5], Step[30/600], Loss: 0.0342\n",
      "Epoch [5/5], Step[40/600], Loss: 0.0170\n",
      "Epoch [5/5], Step[50/600], Loss: 0.0495\n",
      "Epoch [5/5], Step[60/600], Loss: 0.0623\n",
      "Epoch [5/5], Step[70/600], Loss: 0.0618\n",
      "Epoch [5/5], Step[80/600], Loss: 0.0994\n",
      "Epoch [5/5], Step[90/600], Loss: 0.0327\n",
      "Epoch [5/5], Step[100/600], Loss: 0.0719\n",
      "Epoch [5/5], Step[110/600], Loss: 0.0110\n",
      "Epoch [5/5], Step[120/600], Loss: 0.0498\n",
      "Epoch [5/5], Step[130/600], Loss: 0.0422\n",
      "Epoch [5/5], Step[140/600], Loss: 0.0767\n",
      "Epoch [5/5], Step[150/600], Loss: 0.0262\n",
      "Epoch [5/5], Step[160/600], Loss: 0.0181\n",
      "Epoch [5/5], Step[170/600], Loss: 0.0184\n",
      "Epoch [5/5], Step[180/600], Loss: 0.0832\n",
      "Epoch [5/5], Step[190/600], Loss: 0.0379\n",
      "Epoch [5/5], Step[200/600], Loss: 0.0089\n",
      "Epoch [5/5], Step[210/600], Loss: 0.0145\n",
      "Epoch [5/5], Step[220/600], Loss: 0.0312\n",
      "Epoch [5/5], Step[230/600], Loss: 0.0091\n",
      "Epoch [5/5], Step[240/600], Loss: 0.0103\n",
      "Epoch [5/5], Step[250/600], Loss: 0.0705\n",
      "Epoch [5/5], Step[260/600], Loss: 0.0534\n",
      "Epoch [5/5], Step[270/600], Loss: 0.0451\n",
      "Epoch [5/5], Step[280/600], Loss: 0.0459\n",
      "Epoch [5/5], Step[290/600], Loss: 0.0436\n",
      "Epoch [5/5], Step[300/600], Loss: 0.0280\n",
      "Epoch [5/5], Step[310/600], Loss: 0.0189\n",
      "Epoch [5/5], Step[320/600], Loss: 0.0208\n",
      "Epoch [5/5], Step[330/600], Loss: 0.0672\n",
      "Epoch [5/5], Step[340/600], Loss: 0.0387\n",
      "Epoch [5/5], Step[350/600], Loss: 0.0264\n",
      "Epoch [5/5], Step[360/600], Loss: 0.0131\n",
      "Epoch [5/5], Step[370/600], Loss: 0.1333\n",
      "Epoch [5/5], Step[380/600], Loss: 0.0281\n",
      "Epoch [5/5], Step[390/600], Loss: 0.0150\n",
      "Epoch [5/5], Step[400/600], Loss: 0.0295\n",
      "Epoch [5/5], Step[410/600], Loss: 0.0649\n",
      "Epoch [5/5], Step[420/600], Loss: 0.0126\n",
      "Epoch [5/5], Step[430/600], Loss: 0.0577\n",
      "Epoch [5/5], Step[440/600], Loss: 0.0888\n",
      "Epoch [5/5], Step[450/600], Loss: 0.0454\n",
      "Epoch [5/5], Step[460/600], Loss: 0.0872\n",
      "Epoch [5/5], Step[470/600], Loss: 0.0649\n",
      "Epoch [5/5], Step[480/600], Loss: 0.0737\n",
      "Epoch [5/5], Step[490/600], Loss: 0.0110\n",
      "Epoch [5/5], Step[500/600], Loss: 0.0326\n",
      "Epoch [5/5], Step[510/600], Loss: 0.0653\n",
      "Epoch [5/5], Step[520/600], Loss: 0.0343\n",
      "Epoch [5/5], Step[530/600], Loss: 0.0785\n",
      "Epoch [5/5], Step[540/600], Loss: 0.0374\n",
      "Epoch [5/5], Step[550/600], Loss: 0.0240\n",
      "Epoch [5/5], Step[560/600], Loss: 0.0424\n",
      "Epoch [5/5], Step[570/600], Loss: 0.0380\n",
      "Epoch [5/5], Step[580/600], Loss: 0.0393\n",
      "Epoch [5/5], Step[590/600], Loss: 0.0519\n",
      "Epoch [5/5], Step[600/600], Loss: 0.0249\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        images=Variable(images.view(-1,28*28))\n",
    "        labels = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs=net(images)\n",
    "        loss=cirterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(i+1)%10 == 0:\n",
    "            print('Epoch [%d/%d], Step[%d/%d], Loss: %.4f' %(epoch+1, num_epochs,i+1,len(train_dataset)//batch_size, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(100)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(199)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(297)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(395)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(492)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(590)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(685)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(784)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(883)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(977)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(1074)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(1170)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(1266)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(1363)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(1463)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(1558)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(1655)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(1751)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(1850)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(1946)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(2042)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(2135)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(2232)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(2331)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(2427)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(2526)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(2624)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(2723)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(2821)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(2918)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(3016)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(3115)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(3215)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(3315)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(3414)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(3509)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(3607)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(3702)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(3797)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(3892)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(3989)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(4087)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(4182)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(4282)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(4380)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(4478)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(4577)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(4675)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(4768)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(4867)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(4966)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(5065)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(5165)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(5264)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(5363)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(5463)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(5561)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(5659)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(5757)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(5854)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(5949)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(6048)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(6148)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(6248)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(6348)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(6440)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(6538)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(6637)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(6737)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(6837)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(6937)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(7037)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(7136)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(7236)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(7335)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(7435)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(7535)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(7635)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(7735)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(7834)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(7931)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(8030)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(8130)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(8228)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(8325)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(8422)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(8522)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(8622)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(8722)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(8822)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(8918)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(9018)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(9117)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(9217)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(9317)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(9416)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(9509)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(9600)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(9698)\n",
      "predicted\n",
      "torch.Size([100])\n",
      "labels\n",
      "torch.Size([100])\n",
      "tensor(9797)\n",
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    }
   ],
   "source": [
    "correct=0\n",
    "total=0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1,28*28))\n",
    "    outputs=net(images)\n",
    "    #print(outputs.size())\n",
    "    _,predicted=torch.max(outputs.data,1)#return index of max in column\n",
    "    print('predicted')\n",
    "    print(predicted.shape)\n",
    "    print('labels')\n",
    "    print(labels.shape)\n",
    "    total+=labels.size(0)#row\n",
    "    correct+=(predicted==labels).sum()\n",
    "    print(correct)\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' %(100*correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=(r'/home/图片')\n",
    "open(path+'p1.png','r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
